["import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport nni.retiarii.nn.pytorch\n\nimport torch\n\n\nclass _model__fc1__1(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layerchoice__mutation_6_0 = torch.nn.modules.activation.ReLU()\n\n    def forward(self, *_inputs):\n        layerchoice__mutation_6_0 = self.layerchoice__mutation_6_0(_inputs[0])\n        return layerchoice__mutation_6_0\n\n\n\nclass _model__fc1(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.__0 = torch.nn.modules.linear.Linear(in_features=384, out_features=256)\n        self.__1 = _model__fc1__1()\n\n    def forward(self, input__1):\n        __0 = self.__0(input__1)\n        __1 = self.__1(__0)\n        return __1\n\n\n\nclass _model__fc2__1(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layerchoice__mutation_7_3 = torch.nn.modules.activation.Tanh()\n\n    def forward(self, *_inputs):\n        layerchoice__mutation_7_3 = self.layerchoice__mutation_7_3(_inputs[0])\n        return layerchoice__mutation_7_3\n\n\n\nclass _model__fc2(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.__0 = torch.nn.modules.linear.Linear(in_features=256, out_features=64)\n        self.__1 = _model__fc2__1()\n\n    def forward(self, input__1):\n        __0 = self.__0(input__1)\n        __1 = self.__1(__0)\n        return __1\n\n\n\nclass _model__fc3__1(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layerchoice__mutation_8_1 = torch.nn.modules.activation.LeakyReLU()\n\n    def forward(self, *_inputs):\n        layerchoice__mutation_8_1 = self.layerchoice__mutation_8_1(_inputs[0])\n        return layerchoice__mutation_8_1\n\n\n\nclass _model__fc3(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.__0 = torch.nn.modules.linear.Linear(in_features=64, out_features=64)\n        self.__1 = _model__fc3__1()\n\n    def forward(self, input__1):\n        __0 = self.__0(input__1)\n        __1 = self.__1(__0)\n        return __1\n\n\n\nclass _model__fc4__1(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layerchoice__mutation_9_1 = torch.nn.modules.activation.LeakyReLU()\n\n    def forward(self, *_inputs):\n        layerchoice__mutation_9_1 = self.layerchoice__mutation_9_1(_inputs[0])\n        return layerchoice__mutation_9_1\n\n\n\nclass _model__fc4(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.__0 = torch.nn.modules.linear.Linear(in_features=64, out_features=16)\n        self.__1 = _model__fc4__1()\n\n    def forward(self, input__1):\n        __0 = self.__0(input__1)\n        __1 = self.__1(__0)\n        return __1\n\n\n\nclass _model__final_layer(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.__0 = torch.nn.modules.linear.Linear(in_features=16, out_features=2)\n        self.__1 = torch.nn.modules.activation.Softmax()\n\n    def forward(self, input__1):\n        __0 = self.__0(input__1)\n        __1 = self.__1(__0)\n        return __1\n\n\n\nclass _model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.__fc1 = _model__fc1()\n        self.__fc2 = _model__fc2()\n        self.__fc3 = _model__fc3()\n        self.__fc4 = _model__fc4()\n        self.__final_layer = _model__final_layer()\n\n    def forward(self, x__1):\n        __Constant1 = 2\n        __Constant2 = 3\n        __Constant3 = 4\n        __Attr7 = 4\n        __Attr13 = 4\n        __Attr19 = 4\n        __fc1 = self.__fc1(x__1)\n        __aten__ge8 = (__Attr7 >= __Constant1)\n        __aten__ge14 = (__Attr13 >= __Constant2)\n        __aten__ge20 = (__Attr19 >= __Constant3)\n        __fc2 = self.__fc2(__fc1)\n        __noop_identity12 = __fc2\n        __fc3 = self.__fc3(__noop_identity12)\n        __noop_identity18 = __fc3\n        __fc4 = self.__fc4(__noop_identity18)\n        __noop_identity24 = __fc4\n        __final_layer = self.__final_layer(__noop_identity24)\n        return __final_layer"]