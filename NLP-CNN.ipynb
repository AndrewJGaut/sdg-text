{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLP-CNN.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyNBdcb7ZkVn+4zdrF/fqydZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"83d6d5ec79354edea14196865e157382":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_3fe88a7242e84460bed59302128054e2","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_6510dfa426ee446ab6519826957dad21","IPY_MODEL_5bdca0c853a24fdc8682fb886068e379","IPY_MODEL_b91f1429b9374669a66f76ef9bf4af13"]}},"3fe88a7242e84460bed59302128054e2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6510dfa426ee446ab6519826957dad21":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_0b80cbe672f843b6b4ed2e4201b8d5ec","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_16aa7bc9bdb2420a94dd74435010ec4c"}},"5bdca0c853a24fdc8682fb886068e379":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_dccf1cef4ff04a4b8710b58e8bc677f8","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_79b476fbdddd413081ef43410446732e"}},"b91f1429b9374669a66f76ef9bf4af13":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_be63994d061142cab85766a66aadacab","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1999995/? [00:28&lt;00:00, 68615.92it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8c5866aa0f2a46c5957191f61ad575a5"}},"0b80cbe672f843b6b4ed2e4201b8d5ec":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"16aa7bc9bdb2420a94dd74435010ec4c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"dccf1cef4ff04a4b8710b58e8bc677f8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"79b476fbdddd413081ef43410446732e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":"20px","min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"be63994d061142cab85766a66aadacab":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"8c5866aa0f2a46c5957191f61ad575a5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"3uvpukO93fOa"},"source":["import os\n","import re\n","from tqdm import tqdm\n","import numpy as np\n","import pandas as pd\n","import nltk\n","nltk.download(\"all\")\n","import nltk\n","nltk.download('punkt')\n","\n","import matplotlib.pyplot as plt\n","import torch\n","\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9bAAwQnMNnzi","executionInfo":{"status":"ok","timestamp":1638547166003,"user_tz":480,"elapsed":966,"user":{"displayName":"Won Kyung Do","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11548763418747292778"}},"outputId":"359f4c86-afce-4f6c-eb7d-196efeaf10c9"},"source":["from google.colab import drive\n","\n","#get datasets\n","drive.mount('/content/drive', force_remount=True)\n","\n","# change the foldername as the project folder location\n","FOLDERNAME ='/content/drive/MyDrive/CS329P/Project'\n","\n","# assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n","# %cd /content/drive/MyDrive\n","%cd $FOLDERNAME"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/CS329P/Project\n"]}]},{"cell_type":"code","metadata":{"id":"HEqEqOLqOGDz","executionInfo":{"status":"ok","timestamp":1638547171107,"user_tz":480,"elapsed":273,"user":{"displayName":"Won Kyung Do","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11548763418747292778"}}},"source":["def load_text(path, type):\n","\n","    \"\"\"\n","    Load text data, convert all into lowercase text and save to a list.\n","    \"\"\"\n","    if type == 1:\n","\n","        with open(path, 'rb') as f:\n","            texts = []\n","            for line in f:\n","                texts.append(line.decode(errors='ignore').lower().strip())\n","    if type == 2:\n","        with open(path, 'rb') as f:\n","            texts = []\n","            for line in f:\n","                texts.append(int(line.decode(errors='ignore').lower().strip()))  \n","    return texts\n","\n","# Load files\n","data_text = load_text('data_cnn/asset_indexdata.txt', 1)\n","labels = load_text('data_cnn/asset_indexlabel.txt',2)\n","# convert into np array\n","texts = np.array(data_text)\n","labels = np.array(labels)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gvcJgmxXU-q5","executionInfo":{"status":"ok","timestamp":1638547172700,"user_tz":480,"elapsed":547,"user":{"displayName":"Won Kyung Do","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11548763418747292778"}},"outputId":"ad9e27f0-0925-479e-ee66-872907c1076b"},"source":["print(texts, labels)"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["['as such during the late ottoman era, upper reka males (mainly adults) would seasonally go on kurbet or economic migration.. often they would find employment as pastry makers or as halva, salep and boza merchants and salesmen in the then ottoman capital istanbul or regional cities like skopje and edirne.'\n"," 'in october 2012, privatization agency of serbia announced that it had won the case against the former owners, meaning they will have to pay €17 million in the name of compensation. however, in the early 2019 city announced new urban regulatory plan for the area where the brewery is, envisioning the conversion of the land from economic to commercial. offered assets include factory complex at mostar (), juice factory, brewery in čačak, commercial offices in obrenovac, kosovska mitrovica and alibunar, retail store in kragujevac, and an apartment in budva, montenegro. bogdan veljković described the entire sale process as criminal, lawless, corruption, claiming the entire worth of the bip was €300 million.'\n"," \"the memorial was also inscribed with the names of its four authors and designers, but also names of the companies which donated money or worked on monument's construction. radović added that the monument wasn't built by marković, not the state, but by the tax payers.\"\n"," ...\n"," 'on this basis there was a significant economic and social prestrukturalizacija households, as can be seen from the statistics: in 1971 the village had a population of 15 agricultural, 80 mixed and 67 non-agricultural households.'\n"," 'at the same time there was a significant economic and social restructuring of the population (in 1971 there were 32 agricultural, 104 mixed and 158 non-agricultural households).'\n"," 'it derives its name from a garden that was the property of a certain babić family, and it was located in a street that now has the same name as the local community.'] [1 0 0 ... 0 0 0]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8RHtvXuLBcNY","executionInfo":{"status":"ok","timestamp":1638547255195,"user_tz":480,"elapsed":866,"user":{"displayName":"Won Kyung Do","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11548763418747292778"}},"outputId":"b9945302-c26b-493c-b670-3ff7f96c9f9f"},"source":["print(type(labels[0]), type(labels1[0]))\n","print(type(texts[0]), type(texts1[0]))\n","print(type(labels), type(labels1))\n","print(type(texts), type(texts1))"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'numpy.int64'> <class 'numpy.int64'>\n","<class 'numpy.str_'> <class 'numpy.str_'>\n","<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n","<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"]}]},{"cell_type":"markdown","metadata":{"id":"8f-rdonBBIu2"},"source":["Let's use pretrained word vectors(FastText Word Vectors, Mikolov et al., 2017) to train the tokens"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"28MnN5yOBDab","executionInfo":{"status":"ok","timestamp":1638547255671,"user_tz":480,"elapsed":6,"user":{"displayName":"Won Kyung Do","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11548763418747292778"}},"outputId":"6f8d8675-9a65-4531-987f-c0608da76074"},"source":["%%time\n","URL = \"https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip\"\n","FILE = \"fastText\"\n","\n","if os.path.isdir(FILE):\n","    print(\"fastText exists.\")\n","else:\n","    !wget -P $FILE $URL\n","    !unzip $FILE/crawl-300d-2M.vec.zip -d $FILE\n","\n"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["fastText exists.\n","CPU times: user 0 ns, sys: 441 µs, total: 441 µs\n","Wall time: 482 µs\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qb5HvwhDBhET","executionInfo":{"status":"ok","timestamp":1638547255671,"user_tz":480,"elapsed":3,"user":{"displayName":"Won Kyung Do","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11548763418747292778"}},"outputId":"965ba547-d31a-462e-b911-9e65577b8ee4"},"source":["if torch.cuda.is_available():       \n","    device = torch.device(\"cuda\")\n","    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n","    print('Device name:', torch.cuda.get_device_name(0))\n","\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["There are 1 GPU(s) available.\n","Device name: Tesla P100-PCIE-16GB\n"]}]},{"cell_type":"markdown","metadata":{"id":"W5ukK7vWKC37"},"source":["# Tokenizing\n","\n","Not sure whether we need this process or not - Cara already made some codes for this"]},{"cell_type":"code","metadata":{"id":"CI_vBHRVBo9K","executionInfo":{"status":"ok","timestamp":1638547256696,"user_tz":480,"elapsed":3,"user":{"displayName":"Won Kyung Do","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11548763418747292778"}}},"source":["\n","from tqdm import tqdm_notebook\n","from collections import defaultdict\n","from nltk.tokenize import word_tokenize\n","from torch.utils.data import (TensorDataset, DataLoader, RandomSampler, SequentialSampler)\n","from sklearn.model_selection import train_test_split\n","\n","\n","def tokenize(texts):\n","    \"\"\"\n","    1. Tokenize\n","    2. build vocabulary\n","    3. save maximum sentence length \n","    Output:\n","        tokenized \n","        word2idx (Dict): Vocabulary built from the corpus\n","        max_len \n","    \"\"\"\n","    max_len = 0\n","    tokenized_texts = []\n","    word2idx = {}\n","\n","    # Add <pad> and <unk> tokens to the vocabulary\n","    word2idx['<pad>'] = 0\n","    word2idx['<unk>'] = 1\n","\n","    # Building our vocab from the corpus starting from index 2\n","    idx = 2\n","    for sent in texts:\n","        tokenized_sent = word_tokenize(sent)\n","\n","        # Add `tokenized_sent` to `tokenized_texts`\n","        tokenized_texts.append(tokenized_sent)\n","\n","        # Add new token to `word2idx`\n","        for token in tokenized_sent:\n","            if token not in word2idx:\n","                word2idx[token] = idx\n","                idx += 1\n","\n","        # Update `max_len`\n","        max_len = max(max_len, len(tokenized_sent))\n","\n","    return tokenized_texts, word2idx, max_len\n","\n","def encode(tokenized_texts, word2idx, max_len):\n","    \"\"\"\n","    First, we need to pad each sentence to the number of maximum sentence length\n","    Next, encode tokens to the index \n","\n","    Output: Input for the model - array of token indexes in the vocabulary, (N, max_len)\n","    \"\"\"\n","\n","    input_ids = []\n","    for tokenized_sent in tokenized_texts:\n","        # Pad sentences to max_len\n","        tokenized_sent += ['<pad>'] * (max_len - len(tokenized_sent))\n","\n","        # Encode tokens to input_ids\n","        input_id = [word2idx.get(token) for token in tokenized_sent]\n","        input_ids.append(input_id)\n","    \n","    return np.array(input_ids)\n","\n","def load_pretrained_vectors(word2idx, fname):\n","    \"\"\"\n","    Input:\n","        word2idx (Dict): Vocabulary built from the corpus\n","        fname: Path to pretrained vector file\n","\n","    Output:\n","        embeddings (np.array): Embedding matrix with shape (N, d) where N is\n","            the size of word2idx and d is embedding dimension\n","    \"\"\"\n","\n","    print(\"Loading pretrained vectors...\")\n","    fin = open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n","    n, d = map(int, fin.readline().split())\n","\n","\n","    embeddings = np.random.uniform(-0.25, 0.25, (len(word2idx), d))\n","    embeddings[word2idx['<pad>']] = np.zeros((d,))\n","\n","    # Load pretrained vectors\n","    count = 0\n","    for line in tqdm_notebook(fin):\n","        tokens = line.rstrip().split(' ')\n","        word = tokens[0]\n","        if word in word2idx:\n","            count += 1\n","            embeddings[word2idx[word]] = np.array(tokens[1:], dtype=np.float32)\n","\n","    print(f\"There are {count} / {len(word2idx)} pretrained vectors found.\")\n","\n","    return embeddings"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7r0Bko9jm0UH"},"source":[""]},{"cell_type":"code","metadata":{"id":"LN2kYux9nrt_","executionInfo":{"status":"ok","timestamp":1638547256697,"user_tz":480,"elapsed":4,"user":{"displayName":"Won Kyung Do","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11548763418747292778"}}},"source":[""],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"tDOCBEZtKJnE","colab":{"base_uri":"https://localhost:8080/","height":160,"referenced_widgets":["83d6d5ec79354edea14196865e157382","3fe88a7242e84460bed59302128054e2","6510dfa426ee446ab6519826957dad21","5bdca0c853a24fdc8682fb886068e379","b91f1429b9374669a66f76ef9bf4af13","0b80cbe672f843b6b4ed2e4201b8d5ec","16aa7bc9bdb2420a94dd74435010ec4c","dccf1cef4ff04a4b8710b58e8bc677f8","79b476fbdddd413081ef43410446732e","be63994d061142cab85766a66aadacab","8c5866aa0f2a46c5957191f61ad575a5"]},"executionInfo":{"status":"ok","timestamp":1638552807151,"user_tz":480,"elapsed":31974,"user":{"displayName":"Won Kyung Do","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11548763418747292778"}},"outputId":"0d28ac0e-f618-4ac2-fe3e-457f9636b184"},"source":["import nltk\n","nltk.download('punkt')\n","\n","################ Tokenize\n","tokenized_texts, word2idx, max_len = tokenize(texts)\n","input_ids = encode(tokenized_texts, word2idx, max_len)\n","\n","# Pretrained vectors\n","embeddings = load_pretrained_vectors(word2idx, \"fastText/crawl-300d-2M.vec\")\n","embeddings = torch.tensor(embeddings)\n"],"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","Loading pretrained vectors...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:89: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n","Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"83d6d5ec79354edea14196865e157382","version_minor":0,"version_major":2},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["There are 6335 / 8130 pretrained vectors found.\n"]}]},{"cell_type":"code","metadata":{"id":"cASKfTFhnBpP","executionInfo":{"status":"ok","timestamp":1638552807152,"user_tz":480,"elapsed":23,"user":{"displayName":"Won Kyung Do","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11548763418747292778"}}},"source":["\n","def data_loader(train_inputs, val_inputs, train_labels, val_labels,\n","                batch_size=50):\n","\n","    train_inputs, val_inputs, train_labels, val_labels =\\\n","    tuple(torch.tensor(data) for data in\n","          [train_inputs, val_inputs, train_labels, val_labels])\n","\n","    ############ batch\n","    batch_size = 50\n","\n","    # DataLoader for training data\n","    train_data = TensorDataset(train_inputs, train_labels)\n","    train_sampler = RandomSampler(train_data)\n","    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","    # DataLoader for validation data\n","    val_data = TensorDataset(val_inputs, val_labels)\n","    val_sampler = SequentialSampler(val_data)\n","    val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n","\n","    return train_dataloader, val_dataloader"],"execution_count":40,"outputs":[]},{"cell_type":"code","metadata":{"id":"0WaW51cYnIW1","executionInfo":{"status":"ok","timestamp":1638552807153,"user_tz":480,"elapsed":22,"user":{"displayName":"Won Kyung Do","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11548763418747292778"}}},"source":["# Train Test Split\n","train_inputs, val_inputs, train_labels, val_labels = train_test_split(\n","    input_ids, labels, test_size=0.1, random_state=42)\n","\n","# Load data to PyTorch DataLoader\n","train_dataloader, val_dataloader = data_loader(train_inputs, val_inputs, train_labels, val_labels, batch_size=50)"],"execution_count":41,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GAh5zss3ni87"},"source":[""]},{"cell_type":"code","metadata":{"id":"dtoj0nYXnPpz","executionInfo":{"status":"ok","timestamp":1638552807154,"user_tz":480,"elapsed":22,"user":{"displayName":"Won Kyung Do","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11548763418747292778"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import random\n","import time\n","\n","# Sample configuration:\n","filter_sizes = [2, 3, 4]\n","num_filters = [2, 2, 2]\n","\n","\n","# 1D CNN\n","class CNN_NLP(nn.Module):\n","    def __init__(self,\n","                 pretrained_embedding=None,\n","                 freeze_embedding=False,\n","                 vocab_size=None,\n","                 embed_dim=300,\n","                 filter_sizes=[3, 4, 5],\n","                 num_filters=[100, 100, 100],\n","                 num_classes=2,\n","                 dropout=0.5):\n","        \"\"\"\n","            pretrained_embedding: (vocab_size, embed_dim)\n","\n","            When pretrained word embeddings are not used\n","                vocab_size,  embed_dim\n","            n_classes: for now, we simplified the class as two (idx = 1 or idx = 2, because higher index has small # of dataset)\n","            dropout rate\n","        \"\"\"\n","\n","        super(CNN_NLP, self).__init__()\n","        \n","        \n","        # Embedding layer when we use pretrained model \n","        if pretrained_embedding is not None:\n","            self.vocab_size, self.embed_dim = pretrained_embedding.shape\n","            self.embedding = nn.Embedding.from_pretrained(pretrained_embedding,\n","                                                          freeze=freeze_embedding)\n","        else:\n","            self.embed_dim = embed_dim\n","            self.embedding = nn.Embedding(num_embeddings=vocab_size,\n","                                          embedding_dim=self.embed_dim,\n","                                          padding_idx=0,\n","                                          max_norm=5.0)\n","        # Conv Network\n","        self.conv1d_list = nn.ModuleList([\n","            nn.Conv1d(in_channels=self.embed_dim,\n","                      out_channels=num_filters[i],\n","                      kernel_size=filter_sizes[i])\n","            for i in range(len(filter_sizes))\n","        ])\n","        # Fully-connected layer and Dropout\n","        self.fc = nn.Linear(np.sum(num_filters), num_classes)\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","    def forward(self, input_ids):\n","\n","\n","        #  (b, max_len, embed_dim)\n","        embed_frominput = self.embedding(input_ids).float()\n","\n","        #  (b, embed_dim, max_len)\n","        embed_reshaped = embed_frominput.permute(0, 2, 1)\n","\n","        # CNN & ReLU: (b, num_filters[i], L_out)\n","        embed_conv_list = [F.relu(conv1d(embed_reshaped)) for conv1d in self.conv1d_list]\n","\n","        # Max pool: (b, num_filters[i], 1)\n","        embed_pool_list = [F.max_pool1d(x_conv, kernel_size=x_conv.shape[2])\n","            for x_conv in embed_conv_list]\n","        \n","        # FC- (b, sum(num_filters))\n","        embed_fc = torch.cat([x_pool.squeeze(dim=2) for x_pool in embed_pool_list],\n","                         dim=1)\n","        \n","        # (b, n_classes)\n","        output = self.fc(self.dropout(embed_fc))\n","\n","        return output"],"execution_count":42,"outputs":[]},{"cell_type":"code","metadata":{"id":"3AtwTvzVnWNq","executionInfo":{"status":"ok","timestamp":1638552807154,"user_tz":480,"elapsed":19,"user":{"displayName":"Won Kyung Do","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11548763418747292778"}}},"source":["def initialize_model(pretrained_embedding=None, freeze_embedding=False,\n","                    vocab_size=None,  embed_dim=300,\n","                    filter_sizes=[3, 4, 5],\n","                    num_filters=[100, 100, 100],\n","                    num_classes=2,\n","                    dropout=0.5,\n","                    learning_rate=0.01):\n","\n","    assert (len(filter_sizes) == len(num_filters)), \"filter_sizes and num_filters are not the same length!\"\n","\n","    cnn_model = CNN_NLP(pretrained_embedding=pretrained_embedding,\n","                        freeze_embedding=freeze_embedding,\n","                        vocab_size=vocab_size,\n","                        embed_dim=embed_dim,\n","                        filter_sizes=filter_sizes,\n","                        num_filters=num_filters,\n","                        num_classes=2,\n","                        dropout=0.5)\n","    \n","    cnn_model.to(device)\n","\n","    # since it is adam, maybe we need to decrease lr into 0.0001?\n","    optimizer = optim.Adam(cnn_model.parameters(),\n","                               lr=learning_rate)\n","\n","    return cnn_model, optimizer\n","\n","\n","loss_fn = nn.CrossEntropyLoss()\n","\n","# for reproducability, set random seed\n","def set_seed(seed_value=32):\n","    random.seed(seed_value)\n","    np.random.seed(seed_value)\n","    torch.manual_seed(seed_value)\n","    torch.cuda.manual_seed_all(seed_value)\n","\n","\n","def train(model, optimizer, train_dataloader, val_dataloader=None, epochs=10):\n","    \"\"\"Train the CNN model.\"\"\"\n","    \n","    # Tracking best validation accuracy\n","    best_accuracy = 0\n","\n","    print(\"Start training!\\n\")\n","\n","    print(f\"{'Epoch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {\\\n","    'Val Acc':^9} | {'Elapsed':^9}\")\n","    print(\"-\"*60)\n","\n","    for epoch_i in range(epochs):\n","\n","\n","        ################## Training ##############\n","\n","        t0_epoch = time.time()\n","        total_loss = 0\n","\n","        # training mode\n","        model.train()\n","\n","        for step, batch in enumerate(train_dataloader):\n","            # Load batch to GPU\n","            tr_input_ids, tr_labels = tuple(t.to(device) for t in batch)\n","\n","            model.zero_grad()\n","            pred_grad = model(tr_input_ids)\n","\n","            loss = loss_fn(pred_grad, tr_labels)\n","            total_loss += loss.item()\n","\n","            loss.backward()\n","\n","            optimizer.step()\n","\n","        # Calculate the average loss over the entire training data\n","        avg_train_loss = total_loss / len(train_dataloader)\n","\n","        ################## Evaluation #######################\n","\n","        if val_dataloader is not None:\n","\n","            val_loss, val_accuracy = evaluate(model, val_dataloader)\n","\n","            # best accuracy\n","            if val_accuracy > best_accuracy:\n","                best_accuracy = val_accuracy\n","\n","            time_elapsed = time.time() - t0_epoch\n","            print(f\"{epoch_i + 1:^7} | {avg_train_loss:^12.6f} | {\\\n","            val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n","            \n","    print(\"\\n\")\n","    print(f\"Training complete! Best accuracy: {best_accuracy:.2f}%.\")\n","\n","def evaluate(model, val_dataloader):\n","    \"\"\"After the completion of each training epoch, measure the model's\n","    performance on our validation set.\n","    \"\"\"\n","\n","    model.eval()\n","\n","    val_accuracy = []\n","    val_loss = []\n","\n","    for batch in val_dataloader:\n","        ev_input_ids, ev_labels = tuple(t.to(device) for t in batch)\n","\n","        with torch.no_grad():\n","            logits = model(ev_input_ids)\n","\n","        loss = loss_fn(logits, ev_labels)\n","        val_loss.append(loss.item())\n","\n","        preds = torch.argmax(logits, dim=1).flatten()\n","\n","        # Accuracy rate from mean\n","        accuracy = (preds == ev_labels).cpu().numpy().mean() * 100\n","        val_accuracy.append(accuracy)\n","\n","    val_loss = np.mean(val_loss)\n","    val_accuracy = np.mean(val_accuracy)\n","\n","    return val_loss, val_accuracy"],"execution_count":43,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_saLGDCJDhTv","executionInfo":{"status":"ok","timestamp":1638552837611,"user_tz":480,"elapsed":30475,"user":{"displayName":"Won Kyung Do","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11548763418747292778"}},"outputId":"f4a38c03-6270-40be-f332-9e10fe22d2c3"},"source":["# are randomly initialized word vectors\n","cnn_rand, optimizer = initialize_model(vocab_size=len(word2idx),\n","                                      embed_dim=300,\n","                                      learning_rate=0.25,\n","                                      dropout=0.5)\n","train(cnn_rand, optimizer, train_dataloader, val_dataloader, epochs=20)"],"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["Start training!\n","\n"," Epoch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","------------------------------------------------------------\n","   1    |  71.563192   | 125.822837 |   89.23   |   1.56   \n","   2    |  300.847950  | 1054.713114 |   63.85   |   1.54   \n","   3    |  322.448506  | 182.718499 |   95.84   |   1.54   \n","   4    |  271.451195  | 417.557353 |   96.22   |   1.54   \n","   5    |  225.392301  | 259.137407 |   96.22   |   1.53   \n","   6    |  290.069156  | 324.013992 |   97.46   |   1.55   \n","   7    |  231.990994  | 587.068177 |   96.89   |   1.53   \n","   8    |  277.651063  | 167.044724 |   98.89   |   1.53   \n","   9    |  378.204497  | 1504.912021 |   96.73   |   1.54   \n","  10    |  306.045608  | 806.691069 |   98.00   |   1.54   \n","  11    |  465.283389  | 600.442376 |   98.44   |   1.54   \n","  12    |  350.056999  | 485.382697 |   99.11   |   1.53   \n","  13    |  226.516322  | 633.442813 |   99.33   |   1.54   \n","  14    |  317.766670  | 998.012990 |   98.67   |   1.54   \n","  15    |  143.835170  | 953.448798 |   99.11   |   1.54   \n","  16    |  216.725094  | 837.402908 |   99.11   |   1.54   \n","  17    |  345.178885  | 793.050025 |   99.33   |   1.53   \n","  18    |  439.112982  | 726.926039 |   99.56   |   1.54   \n","  19    |  549.934874  | 1075.327620 |   99.33   |   1.54   \n","  20    |  541.495950  | 1001.106556 |   99.56   |   1.54   \n","\n","\n","Training complete! Best accuracy: 99.56%.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qf2RBV_5EEkw","executionInfo":{"status":"ok","timestamp":1638552858900,"user_tz":480,"elapsed":21304,"user":{"displayName":"Won Kyung Do","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11548763418747292778"}},"outputId":"1079244c-e9f9-49df-f386-74afd43ad71a"},"source":["# freezed pretrained word vectors during training\n","set_seed(28)\n","cnn_static, optimizer = initialize_model(pretrained_embedding=embeddings,\n","                                        freeze_embedding=True,\n","                                        learning_rate=0.25,\n","                                        dropout=0.5)\n","train(cnn_static, optimizer, train_dataloader, val_dataloader, epochs=20)"],"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["Start training!\n","\n"," Epoch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","------------------------------------------------------------\n","   1    |  20.039886   |  7.277438  |   87.39   |   1.06   \n","   2    |  14.194124   |  7.735140  |   90.41   |   1.06   \n","   3    |  22.819833   | 20.685639  |   90.91   |   1.06   \n","   4    |  33.215984   | 17.257021  |   96.06   |   1.06   \n","   5    |  37.800380   | 27.668060  |   96.51   |   1.06   \n","   6    |  37.595427   | 10.824505  |   98.89   |   1.06   \n","   7    |  19.223685   | 17.624871  |   97.39   |   1.06   \n","   8    |  18.740821   |  9.220479  |   98.89   |   1.06   \n","   9    |  27.766165   | 10.738818  |   99.11   |   1.06   \n","  10    |  38.282627   | 10.148971  |   98.73   |   1.06   \n","  11    |  33.584561   | 17.236425  |   99.11   |   1.07   \n","  12    |  32.105178   | 26.532780  |   98.89   |   1.07   \n","  13    |  27.313973   |  9.601482  |   99.56   |   1.06   \n","  14    |  13.393506   |  9.772540  |   99.33   |   1.07   \n","  15    |  21.133523   |  9.343191  |   99.33   |   1.06   \n","  16    |   8.980784   |  8.607542  |   99.56   |   1.06   \n","  17    |   6.523343   |  3.877252  |   99.56   |   1.06   \n","  18    |  18.642597   |  7.894505  |   99.56   |   1.06   \n","  19    |  10.299101   |  3.684042  |   99.11   |   1.07   \n","  20    |  15.818595   |  9.598615  |   99.33   |   1.07   \n","\n","\n","Training complete! Best accuracy: 99.56%.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t2UmlxJSEHvP","executionInfo":{"status":"ok","timestamp":1638552890728,"user_tz":480,"elapsed":31843,"user":{"displayName":"Won Kyung Do","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11548763418747292778"}},"outputId":"88851d8c-358a-47e2-b2c4-226d302e7225"},"source":["# fine-tuned pretrained word vectors during training \n","set_seed(42)\n","cnn_non_static, optimizer = initialize_model(pretrained_embedding=embeddings,\n","                                            freeze_embedding=False,\n","                                            learning_rate=0.25,\n","                                            dropout=0.5)\n","train(cnn_non_static, optimizer, train_dataloader, val_dataloader, epochs=20)\n"],"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["Start training!\n","\n"," Epoch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","------------------------------------------------------------\n","   1    |  414.144132  | 2906.369646 |   91.17   |   1.59   \n","   2    | 3174.279405  | 3614.241252 |   97.11   |   1.59   \n","   3    | 1929.099525  | 3375.348715 |   99.33   |   1.59   \n","   4    | 1668.012666  | 5754.295736 |   98.44   |   1.59   \n","   5    | 4420.628805  | 26460.582275 |   97.78   |   1.59   \n","   6    | 9958.678696  | 8580.697266 |   99.56   |   1.59   \n","   7    | 6907.567107  | 33186.208767 |   98.73   |   1.59   \n","   8    | 2000.061381  | 21962.451565 |   99.11   |   1.59   \n","   9    |  779.760617  | 25240.001628 |   98.73   |   1.59   \n","  10    | 7229.381028  | 32643.407254 |   98.51   |   1.59   \n","  11    | 7303.048252  | 43878.338135 |   98.67   |   1.59   \n","  12    | 6015.980996  | 47747.253472 |   99.33   |   1.59   \n","  13    | 3031.479755  | 34899.798177 |   99.33   |   1.58   \n","  14    | 4641.112536  | 80103.300347 |   99.33   |   1.59   \n","  15    | 2404.522176  | 69191.812500 |   99.56   |   1.59   \n","  16    | 8190.660031  | 78195.350694 |   99.56   |   1.60   \n","  17    | 2929.805689  | 77141.031250 |   99.56   |   1.59   \n","  18    | 1951.082958  | 62270.451389 |   99.78   |   1.59   \n","  19    | 3590.293707  | 81092.780816 |   99.33   |   1.59   \n","  20    | 1756.710099  | 110825.427083 |   99.11   |   1.59   \n","\n","\n","Training complete! Best accuracy: 99.78%.\n"]}]},{"cell_type":"code","metadata":{"id":"BzcYsfLpEgd3","executionInfo":{"status":"ok","timestamp":1638552890728,"user_tz":480,"elapsed":6,"user":{"displayName":"Won Kyung Do","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11548763418747292778"}}},"source":["def predict(text, model=cnn_non_static.to(\"cpu\"), max_len=62):\n","    \"\"\"Predict probability that a review is positive.\"\"\"\n","\n","    # Tokenize, pad and encode text\n","    tokens = word_tokenize(text.lower())\n","    padded_tokens = tokens + ['<pad>'] * (max_len - len(tokens))\n","    input_id = [word2idx.get(token, word2idx['<unk>']) for token in padded_tokens]\n","\n","    # Convert to PyTorch tensors\n","    input_id = torch.tensor(input_id).unsqueeze(dim=0)\n","\n","    # Compute logits\n","    logits = model.forward(input_id)\n","\n","    #  Compute probability\n","    probs = F.softmax(logits, dim=1).squeeze(dim=0)\n","\n","    print(f\"This review is {probs[1] * 100:.2f}% positive.\")"],"execution_count":47,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cxQ6XFSSJJAh","executionInfo":{"status":"ok","timestamp":1638552890729,"user_tz":480,"elapsed":6,"user":{"displayName":"Won Kyung Do","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11548763418747292778"}},"outputId":"7c652fc7-d4f1-4dad-e11e-ca6bf56e3348"},"source":["predict(\"The lower dam will regulate outflows from the 횉etin main dam and also produce hydroelectric power with a 112 MW capacity via two 56 MW Kaplan turbines.  .\")"],"execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":["This review is 0.00% positive.\n"]}]},{"cell_type":"code","metadata":{"id":"JE2-7KbEJVhn","executionInfo":{"status":"ok","timestamp":1638552890729,"user_tz":480,"elapsed":5,"user":{"displayName":"Won Kyung Do","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11548763418747292778"}}},"source":[""],"execution_count":48,"outputs":[]}]}