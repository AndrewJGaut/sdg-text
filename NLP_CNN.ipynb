{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "162dbf6a72454b718e07408294e1622e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_59e6bf09ff9c4fbbab90a0358c452724",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e81f147bb62f438ba5d7b8e136917759",
              "IPY_MODEL_5ca9013c8dde4ba29b8c8170e0184ccc",
              "IPY_MODEL_372ccf8eaee04e618edf989b3d23bb6c"
            ]
          }
        },
        "59e6bf09ff9c4fbbab90a0358c452724": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e81f147bb62f438ba5d7b8e136917759": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2340c1e133484da2a56af6ac8761b12a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_388221b69154422bb3f7e999c7329e5a"
          }
        },
        "5ca9013c8dde4ba29b8c8170e0184ccc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_78debb9712624f1e8db4ec83501b59a2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3b35ee7721804cce85ea9835d6962bcc"
          }
        },
        "372ccf8eaee04e618edf989b3d23bb6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_bd388d2136eb4003b1024590e76869d7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1999995/? [01:05&lt;00:00, 45558.71it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8a605a32244d48dc9ce8e8c6ce9e5199"
          }
        },
        "2340c1e133484da2a56af6ac8761b12a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "388221b69154422bb3f7e999c7329e5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "78debb9712624f1e8db4ec83501b59a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3b35ee7721804cce85ea9835d6962bcc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": "20px",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bd388d2136eb4003b1024590e76869d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8a605a32244d48dc9ce8e8c6ce9e5199": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uvpukO93fOa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a095ed4-389b-481d-ae45-e71521d38178"
      },
      "source": [
        "import os\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download(\"all\")\n",
        "nltk.download('punkt')\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet31.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bAAwQnMNnzi",
        "outputId": "91541872-a35e-499c-ca90-0f4fd28e506d"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "#get datasets\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# change the foldername as the project folder location\n",
        "FOLDERNAME ='/content/drive/MyDrive/CS329P/Project'\n",
        "\n",
        "# assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
        "\n",
        "%cd $FOLDERNAME"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/CS329P/Project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEqEqOLqOGDz"
      },
      "source": [
        "def load_text(path, type):\n",
        "    \"\"\"\n",
        "    Load text data, convert all into lowercase text and save to a list.\n",
        "    \"\"\"\n",
        "    if type == 1:\n",
        "\n",
        "        with open(path, 'rb') as f:\n",
        "            texts = []\n",
        "            for line in f:\n",
        "                texts.append(line.decode(errors='ignore').lower().strip())\n",
        "    if type == 2:\n",
        "        with open(path, 'rb') as f:\n",
        "            texts = []\n",
        "            for line in f:\n",
        "                texts.append(int(line.decode(errors='ignore').lower().strip()))  \n",
        "    return texts\n",
        "\n",
        "# Load files\n",
        "data_text = load_text('data_cnn/asset_index_data.txt', 1)\n",
        "labels = load_text('data_cnn/asset_index_label.txt',2)\n",
        "# convert into np array\n",
        "texts = np.array(data_text)\n",
        "labels = np.array(labels)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvcJgmxXU-q5",
        "outputId": "3e095368-56aa-4870-ee12-43656b89a0eb"
      },
      "source": [
        "print(texts, labels)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"like many cities in albania, berat comprises an old fortified city filled with churches and mosques painted with grandiose wealth of visible murals and frescos. conversion to islam of the local urban population in berat had increased during this time and part of the newcomer population were also muslim converts who had islamic names and christian surnames.. factors such as tax exemptions for muslim urban craftsmen in exchange for military service drove many of the incoming rural first generation muslim population to berat.. in the modern period, a romani community numbering 200-300 lives in berat and its outskirts whereas others in a few nearby villages, at times living in difficult economic circumstances with some seasonally migrating to greece for work. economy   by the 18th century the economy and society of berat was closely connected to the city's craft guilds partly related to various tax exemptions that existed since the late middle ages. like many cities in albania, berat comprises an old fortified city filled with churches and mosques painted with grandiose wealth of visible murals and frescos. conversion to islam of the local urban population in berat had increased during this time and part of the newcomer population were also muslim converts who had islamic names and christian surnames.. factors such as tax exemptions for muslim urban craftsmen in exchange for military service drove many of the incoming rural first generation muslim population to berat.. in the modern period, a romani community numbering 200-300 lives in berat and its outskirts whereas others in a few nearby villages, at times living in difficult economic circumstances with some seasonally migrating to greece for work. economy   by the 18th century the economy and society of berat was closely connected to the city's craft guilds partly related to various tax exemptions that existed since the late middle ages. like many cities in albania, berat comprises an old fortified city filled with churches and mosques painted with grandiose wealth of visible murals and frescos. conversion to islam of the local urban population in berat had increased during this time and part of the newcomer population were also muslim converts who had islamic names and christian surnames.. factors such as tax exemptions for muslim urban craftsmen in exchange for military service drove many of the incoming rural first generation muslim population to berat.. in the modern period, a romani community numbering 200-300 lives in berat and its outskirts whereas others in a few nearby villages, at times living in difficult economic circumstances with some seasonally migrating to greece for work. economy   by the 18th century the economy and society of berat was closely connected to the city's craft guilds partly related to various tax exemptions that existed since the late middle ages. like many cities in albania, berat comprises an old fortified city filled with churches and mosques painted with grandiose wealth of visible murals and frescos. conversion to islam of the local urban population in berat had increased during this time and part of the newcomer population were also muslim converts who had islamic names and christian surnames.. factors such as tax exemptions for muslim urban craftsmen in exchange for military service drove many of the incoming rural first generation muslim population to berat.. in the modern period, a romani community numbering 200-300 lives in berat and its outskirts whereas others in a few nearby villages, at times living in difficult economic circumstances with some seasonally migrating to greece for work. economy   by the 18th century the economy and society of berat was closely connected to the city's craft guilds partly related to various tax exemptions that existed since the late middle ages. like many cities in albania, berat comprises an old fortified city filled with churches and mosques painted with grandiose wealth of visible murals and frescos. conversion to islam of the local urban population in berat had increased during this time and part of the newcomer population were also muslim converts who had islamic names and christian surnames.. factors such as tax exemptions for muslim urban craftsmen in exchange for military service drove many of the incoming rural first generation muslim population to berat.. in the modern period, a romani community numbering 200-300 lives in berat and its outskirts whereas others in a few nearby villages, at times living in difficult economic circumstances with some seasonally migrating to greece for work. economy   by the 18th century the economy and society of berat was closely connected to the city's craft guilds partly related to various tax exemptions that existed since the late middle ages. like many cities in albania, berat comprises an old fortified city filled with churches and mosques painted with grandiose wealth of visible murals and frescos. conversion to islam of the local urban population in berat had increased during this time and part of the newcomer population were also muslim converts who had islamic names and christian surnames.. factors such as tax exemptions for muslim urban craftsmen in exchange for military service drove many of the incoming rural first generation muslim population to berat.. in the modern period, a romani community numbering 200-300 lives in berat and its outskirts whereas others in a few nearby villages, at times living in difficult economic circumstances with some seasonally migrating to greece for work. economy   by the 18th century the economy and society of berat was closely connected to the city's craft guilds partly related to various tax exemptions that existed since the late middle ages.\"\n",
            " 'economy historically the main economic activity of the village has been the livestock farming, hunting, beekeeping, and medicinal plants sale. economy historically the main economic activity of the village has been the livestock farming, hunting, beekeeping, and medicinal plants sale. economy historically the main economic activity of the village has been the livestock farming, hunting, beekeeping, and medicinal plants sale. economy historically the main economic activity of the village has been the livestock farming, hunting, beekeeping, and medicinal plants sale.'\n",
            " '\"economy and infrastructure despite the richness of natural resources, unemployment is high. many families live in poverty; 32.4\\xa0percent of families in the commune of martanesh are in receipt of economic assistance.\"'\n",
            " ...\n",
            " 'however, due to economic variables, was unable to attend the national competition.'\n",
            " '\"the majority of the federation\\'s military and financial assets went to southern rhodesia, since the british government did not wish to see them fall into the hands of the nationalist leaders, and since southern rhodesia had borne the major expenses of running the federation. the majority of the federation\\'s military and financial assets went to southern rhodesia, since the british government did not wish to see them fall into the hands of the nationalist leaders, and since southern rhodesia had borne the major expenses of running the federation. the majority of the federation\\'s military and financial assets went to southern rhodesia, since the british government did not wish to see them fall into the hands of the nationalist leaders, and since southern rhodesia had borne the major expenses of running the federation. the majority of the federation\\'s military and financial assets went to southern rhodesia, since the british government did not wish to see them fall into the hands of the nationalist leaders, and since southern rhodesia had borne the major expenses of running the federation. the majority of the federation\\'s military and financial assets went to southern rhodesia, since the british government did not wish to see them fall into the hands of the nationalist leaders, and since southern rhodesia had borne the major expenses of running the federation. the majority of the federation\\'s military and financial assets went to southern rhodesia, since the british government did not wish to see them fall into the hands of the nationalist leaders, and since southern rhodesia had borne the major expenses of running the federation. in late january 2013, the zimbabwean finance ministry reported that they had only $217 in their treasury and would apply for donations to finance the coming elections that is estimated to cost us$107 million.kitsepile, nyathi (30 january 2013) zimbabwe has only $217 in the bank, says finance minister: news, africareview.com; retrieved 4 july 2013. in late january 2013, the zimbabwean finance ministry reported that they had only $217 in their treasury and would apply for donations to finance the coming elections that is estimated to cost us$107 million.kitsepile, nyathi (30 january 2013) zimbabwe has only $217 in the bank, says finance minister: news, africareview.com; retrieved 4 july 2013. in late january 2013, the zimbabwean finance ministry reported that they had only $217 in their treasury and would apply for donations to finance the coming elections that is estimated to cost us$107 million.kitsepile, nyathi (30 january 2013) zimbabwe has only $217 in the bank, says finance minister: news, africareview.com; retrieved 4 july 2013. in late january 2013, the zimbabwean finance ministry reported that they had only $217 in their treasury and would apply for donations to finance the coming elections that is estimated to cost us$107 million.kitsepile, nyathi (30 january 2013) zimbabwe has only $217 in the bank, says finance minister: news, africareview.com; retrieved 4 july 2013. in late january 2013, the zimbabwean finance ministry reported that they had only $217 in their treasury and would apply for donations to finance the coming elections that is estimated to cost us$107 million.kitsepile, nyathi (30 january 2013) zimbabwe has only $217 in the bank, says finance minister: news, africareview.com; retrieved 4 july 2013. in late january 2013, the zimbabwean finance ministry reported that they had only $217 in their treasury and would apply for donations to finance the coming elections that is estimated to cost us$107 million.kitsepile, nyathi (30 january 2013) zimbabwe has only $217 in the bank, says finance minister: news, africareview.com; retrieved 4 july 2013. in late january 2013, the zimbabwean finance ministry reported that they had only $217 in their treasury and would apply for donations to finance the coming elections that is estimated to cost us$107 million.kitsepile, nyathi (30 january 2013) zimbabwe has only $217 in the bank, says finance minister: news, africareview.com; retrieved 4 july 2013. in late january 2013, the zimbabwean finance ministry reported that they had only $217 in their treasury and would apply for donations to finance the coming elections that is estimated to cost us$107 million.kitsepile, nyathi (30 january 2013) zimbabwe has only $217 in the bank, says finance minister: news, africareview.com; retrieved 4 july 2013. in late january 2013, the zimbabwean finance ministry reported that they had only $217 in their treasury and would apply for donations to finance the coming elections that is estimated to cost us$107 million.kitsepile, nyathi (30 january 2013) zimbabwe has only $217 in the bank, says finance minister: news, africareview.com; retrieved 4 july 2013.\"'\n",
            " \"this is due to the bushveld igneous complex, an extremely rich saucer-shaped geological formation that stretches over more than 50,000 square kilometers. malboch refused to pay taxes to the transvaal after it was given back to the boers in 1881 by the british, which resulted in a military drive against him by the south african republic (zar). malboch refused to pay taxes to the transvaal after it was given back to the boers in 1881 by the british, which resulted in a military drive against him by the south african republic (zar). this is due to the bushveld igneous complex, an extremely rich saucer-shaped geological formation that stretches over more than 50,000 square kilometers. this is due to the bushveld igneous complex, an extremely rich saucer-shaped geological formation that stretches over more than 50,000 square kilometers. malboch refused to pay taxes to the transvaal after it was given back to the boers in 1881 by the british, which resulted in a military drive against him by the south african republic (zar). malboch refused to pay taxes to the transvaal after it was given back to the boers in 1881 by the british, which resulted in a military drive against him by the south african republic (zar). malboch refused to pay taxes to the transvaal after it was given back to the boers in 1881 by the british, which resulted in a military drive against him by the south african republic (zar). this is due to the bushveld igneous complex, an extremely rich saucer-shaped geological formation that stretches over more than 50,000 square kilometers. malboch refused to pay taxes to the transvaal after it was given back to the boers in 1881 by the british, which resulted in a military drive against him by the south african republic (zar). malboch refused to pay taxes to the transvaal after it was given back to the boers in 1881 by the british, which resulted in a military drive against him by the south african republic (zar). malboch refused to pay taxes to the transvaal after it was given back to the boers in 1881 by the british, which resulted in a military drive against him by the south african republic (zar). this is due to the bushveld igneous complex, an extremely rich saucer-shaped geological formation that stretches over more than 50,000 square kilometers. malboch refused to pay taxes to the transvaal after it was given back to the boers in 1881 by the british, which resulted in a military drive against him by the south african republic (zar). this is due to the bushveld igneous complex, an extremely rich saucer-shaped geological formation that stretches over more than 50,000 square kilometers. this is due to the bushveld igneous complex, an extremely rich saucer-shaped geological formation that stretches over more than 50,000 square kilometers. this is due to the bushveld igneous complex, an extremely rich saucer-shaped geological formation that stretches over more than 50,000 square kilometers. many villages experience high unemployment, severe poverty or low levels of income despite the area's natural resources. many villages experience high unemployment, severe poverty or low levels of income despite the area's natural resources. many villages experience high unemployment, severe poverty or low levels of income despite the area's natural resources.\"] [0 0 0 ... 1 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f-rdonBBIu2"
      },
      "source": [
        "Let's use pretrained word vectors(FastText Word Vectors, Mikolov et al., 2017) to train the tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28MnN5yOBDab",
        "outputId": "6566c5cf-991d-4501-90fa-4baa6b5a12c7"
      },
      "source": [
        "%%time\n",
        "URL = \"https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip\"\n",
        "FILE = \"fastText\"\n",
        "\n",
        "if os.path.isdir(FILE):\n",
        "    print(\"fastText exists.\")\n",
        "else:\n",
        "    !wget -P $FILE $URL\n",
        "    !unzip $FILE/crawl-300d-2M.vec.zip -d $FILE\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fastText exists.\n",
            "CPU times: user 1.09 ms, sys: 17 µs, total: 1.11 ms\n",
            "Wall time: 730 µs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qb5HvwhDBhET",
        "outputId": "8d8fd078-fa73-4b5a-9603-3cb4d5e7e7d3"
      },
      "source": [
        "if torch.cuda.is_available():       \n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
        "    print('Device name:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "Device name: Tesla P100-PCIE-16GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5ukK7vWKC37"
      },
      "source": [
        "# Tokenizing\n",
        "\n",
        "Not sure whether we need this process or not - Cara already made some codes for this"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CI_vBHRVBo9K"
      },
      "source": [
        "\n",
        "from tqdm import tqdm_notebook\n",
        "from collections import defaultdict\n",
        "from nltk.tokenize import word_tokenize\n",
        "from torch.utils.data import (TensorDataset, DataLoader, RandomSampler, SequentialSampler)\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def tokenize(texts):\n",
        "    \"\"\"\n",
        "    1. Tokenize\n",
        "    2. build vocabulary\n",
        "    3. save maximum sentence length \n",
        "    Output:\n",
        "        tokenized \n",
        "        word2idx (Dict): Vocabulary built from the corpus\n",
        "        max_len \n",
        "    \"\"\"\n",
        "    max_len = 0\n",
        "    tokenized_texts = []\n",
        "    word2idx = {}\n",
        "\n",
        "    # Add <pad> and <unk> tokens to the vocabulary\n",
        "    word2idx['<pad>'] = 0\n",
        "    word2idx['<unk>'] = 1\n",
        "\n",
        "    # Building our vocab from the corpus starting from index 2\n",
        "    idx = 2\n",
        "    for sent in texts:\n",
        "        tokenized_sent = word_tokenize(sent)\n",
        "\n",
        "        # Add `tokenized_sent` to `tokenized_texts`\n",
        "        tokenized_texts.append(tokenized_sent)\n",
        "\n",
        "        # Add new token to `word2idx`\n",
        "        for token in tokenized_sent:\n",
        "            if token not in word2idx:\n",
        "                word2idx[token] = idx\n",
        "                idx += 1\n",
        "\n",
        "        # Update `max_len`\n",
        "        max_len = max(max_len, len(tokenized_sent))\n",
        "\n",
        "    return tokenized_texts, word2idx, max_len\n",
        "\n",
        "def encode(tokenized_texts, word2idx, max_len):\n",
        "    \"\"\"\n",
        "    First, we need to pad each sentence to the number of maximum sentence length\n",
        "    Next, encode tokens to the index \n",
        "\n",
        "    Output: Input for the model - array of token indexes in the vocabulary, (N, max_len)\n",
        "    \"\"\"\n",
        "\n",
        "    input_ids = []\n",
        "    for tokenized_sent in tokenized_texts:\n",
        "        # Pad sentences to max_len\n",
        "        tokenized_sent += ['<pad>'] * (max_len - len(tokenized_sent))\n",
        "\n",
        "        # Encode tokens to input_ids\n",
        "        input_id = [word2idx.get(token) for token in tokenized_sent]\n",
        "        input_ids.append(input_id)\n",
        "    \n",
        "    return np.array(input_ids)\n",
        "\n",
        "def load_pretrained_vectors(word2idx, fname):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        word2idx (Dict): Vocabulary built from the corpus\n",
        "        fname: Path to pretrained vector file\n",
        "\n",
        "    Output:\n",
        "        embeddings (np.array): Embedding matrix with shape (N, d) where N is\n",
        "            the size of word2idx and d is embedding dimension\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Loading pretrained vectors...\")\n",
        "    fin = open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
        "    n, d = map(int, fin.readline().split())\n",
        "\n",
        "\n",
        "    embeddings = np.random.uniform(-0.25, 0.25, (len(word2idx), d))\n",
        "    embeddings[word2idx['<pad>']] = np.zeros((d,))\n",
        "\n",
        "    # Load pretrained vectors\n",
        "    count = 0\n",
        "    for line in tqdm_notebook(fin):\n",
        "        tokens = line.rstrip().split(' ')\n",
        "        word = tokens[0]\n",
        "        if word in word2idx:\n",
        "            count += 1\n",
        "            embeddings[word2idx[word]] = np.array(tokens[1:], dtype=np.float32)\n",
        "\n",
        "    print(f\"There are {count} / {len(word2idx)} pretrained vectors found.\")\n",
        "\n",
        "    return embeddings"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDOCBEZtKJnE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160,
          "referenced_widgets": [
            "162dbf6a72454b718e07408294e1622e",
            "59e6bf09ff9c4fbbab90a0358c452724",
            "e81f147bb62f438ba5d7b8e136917759",
            "5ca9013c8dde4ba29b8c8170e0184ccc",
            "372ccf8eaee04e618edf989b3d23bb6c",
            "2340c1e133484da2a56af6ac8761b12a",
            "388221b69154422bb3f7e999c7329e5a",
            "78debb9712624f1e8db4ec83501b59a2",
            "3b35ee7721804cce85ea9835d6962bcc",
            "bd388d2136eb4003b1024590e76869d7",
            "8a605a32244d48dc9ce8e8c6ce9e5199"
          ]
        },
        "outputId": "452ca363-ecb0-4793-a56d-d3261b3304d7"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "################ Tokenize\n",
        "tokenized_texts, word2idx, max_len = tokenize(texts)\n",
        "input_ids = encode(tokenized_texts, word2idx, max_len)\n",
        "\n",
        "# Pretrained vectors\n",
        "embeddings = load_pretrained_vectors(word2idx, \"fastText/crawl-300d-2M.vec\")\n",
        "embeddings = torch.tensor(embeddings)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Loading pretrained vectors...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:86: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "162dbf6a72454b718e07408294e1622e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 15131 / 20776 pretrained vectors found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cASKfTFhnBpP"
      },
      "source": [
        "\n",
        "def data_loader(train_inputs, val_inputs, train_labels, val_labels,\n",
        "                batch_size=50):\n",
        "\n",
        "    train_inputs, val_inputs, train_labels, val_labels =\\\n",
        "    tuple(torch.tensor(data) for data in\n",
        "          [train_inputs, val_inputs, train_labels, val_labels])\n",
        "\n",
        "    ############ batch\n",
        "    batch_size = 50\n",
        "\n",
        "    # DataLoader for training data\n",
        "    train_data = TensorDataset(train_inputs, train_labels)\n",
        "    train_sampler = RandomSampler(train_data)\n",
        "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "    # DataLoader for validation data\n",
        "    val_data = TensorDataset(val_inputs, val_labels)\n",
        "    val_sampler = SequentialSampler(val_data)\n",
        "    val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
        "\n",
        "    return train_dataloader, val_dataloader"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WaW51cYnIW1"
      },
      "source": [
        "# Train Test Split\n",
        "train_inputs, val_inputs, train_labels, val_labels = train_test_split(\n",
        "    input_ids, labels, test_size=0.1, random_state=42)\n",
        "\n",
        "# Load data to PyTorch DataLoader\n",
        "train_dataloader, val_dataloader = data_loader(train_inputs, val_inputs, train_labels, val_labels, batch_size=50)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAh5zss3ni87"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtoj0nYXnPpz"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import time\n",
        "\n",
        "# Sample configuration:\n",
        "filter_sizes = [2, 3, 4]\n",
        "num_filters = [2, 2, 2]\n",
        "\n",
        "\n",
        "# 1D CNN\n",
        "class CNN_NLP(nn.Module):\n",
        "    def __init__(self,\n",
        "                 pretrained_embedding=None,\n",
        "                 freeze_embedding=False,\n",
        "                 vocab_size=None,\n",
        "                 embed_dim=300,\n",
        "                 filter_sizes=[3, 4, 5],\n",
        "                 num_filters=[100, 100, 100],\n",
        "                 num_classes=2,\n",
        "                 dropout=0.5):\n",
        "        \"\"\"\n",
        "            pretrained_embedding: (vocab_size, embed_dim)\n",
        "\n",
        "            When pretrained word embeddings are not used\n",
        "                vocab_size,  embed_dim\n",
        "            n_classes: for now, we simplified the class as two (idx = 1 or idx = 2, because higher index has small # of dataset)\n",
        "            dropout rate\n",
        "        \"\"\"\n",
        "\n",
        "        super(CNN_NLP, self).__init__()\n",
        "        \n",
        "        \n",
        "        # Embedding layer when we use pretrained model \n",
        "        if pretrained_embedding is not None:\n",
        "            self.vocab_size, self.embed_dim = pretrained_embedding.shape\n",
        "            self.embedding = nn.Embedding.from_pretrained(pretrained_embedding,\n",
        "                                                          freeze=freeze_embedding)\n",
        "        else:\n",
        "            self.embed_dim = embed_dim\n",
        "            self.embedding = nn.Embedding(num_embeddings=vocab_size,\n",
        "                                          embedding_dim=self.embed_dim,\n",
        "                                          padding_idx=0,\n",
        "                                          max_norm=5.0)\n",
        "        # Conv Network\n",
        "        self.conv1d_list = nn.ModuleList([\n",
        "            nn.Conv1d(in_channels=self.embed_dim,\n",
        "                      out_channels=num_filters[i],\n",
        "                      kernel_size=filter_sizes[i])\n",
        "            for i in range(len(filter_sizes))\n",
        "        ])\n",
        "        # Fully-connected layer and Dropout\n",
        "        self.fc = nn.Linear(np.sum(num_filters), num_classes)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "\n",
        "\n",
        "        #  (b, max_len, embed_dim)\n",
        "        embed_frominput = self.embedding(input_ids).float()\n",
        "\n",
        "        #  (b, embed_dim, max_len)\n",
        "        embed_reshaped = embed_frominput.permute(0, 2, 1)\n",
        "\n",
        "        # CNN & ReLU: (b, num_filters[i], L_out)\n",
        "        embed_conv_list = [F.relu(conv1d(embed_reshaped)) for conv1d in self.conv1d_list]\n",
        "\n",
        "        # Max pool: (b, num_filters[i], 1)\n",
        "        embed_pool_list = [F.max_pool1d(x_conv, kernel_size=x_conv.shape[2])\n",
        "            for x_conv in embed_conv_list]\n",
        "        \n",
        "        # FC- (b, sum(num_filters))\n",
        "        embed_fc = torch.cat([x_pool.squeeze(dim=2) for x_pool in embed_pool_list],\n",
        "                         dim=1)\n",
        "        \n",
        "        # (b, n_classes)\n",
        "        output = self.fc(self.dropout(embed_fc))\n",
        "\n",
        "        return output"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AtwTvzVnWNq"
      },
      "source": [
        "def initialize_model(pretrained_embedding=None, freeze_embedding=False,\n",
        "                    vocab_size=None,  embed_dim=300,\n",
        "                    filter_sizes=[3, 4, 5],\n",
        "                    num_filters=[100, 100, 100],\n",
        "                    num_classes=2,\n",
        "                    dropout=0.5,\n",
        "                    learning_rate=0.01):\n",
        "\n",
        "    assert (len(filter_sizes) == len(num_filters)), \"filter_sizes and num_filters are not the same length!\"\n",
        "\n",
        "    cnn_model = CNN_NLP(pretrained_embedding=pretrained_embedding,\n",
        "                        freeze_embedding=freeze_embedding,\n",
        "                        vocab_size=vocab_size,\n",
        "                        embed_dim=embed_dim,\n",
        "                        filter_sizes=filter_sizes,\n",
        "                        num_filters=num_filters,\n",
        "                        num_classes=2,\n",
        "                        dropout=0.5)\n",
        "    \n",
        "    cnn_model.to(device)\n",
        "\n",
        "    # since it is adam, maybe we need to decrease lr into 0.0001?\n",
        "    optimizer = optim.Adam(cnn_model.parameters(),\n",
        "                               lr=learning_rate)\n",
        "\n",
        "    return cnn_model, optimizer\n",
        "\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# for reproducability, set random seed\n",
        "def set_seed(seed_value=32):\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value)\n",
        "\n",
        "\n",
        "def train(model, optimizer, train_dataloader, val_dataloader=None, epochs=10):\n",
        "    \"\"\"Train the CNN model.\"\"\"\n",
        "    \n",
        "    # Tracking best validation accuracy\n",
        "    best_accuracy = 0\n",
        "\n",
        "    print(\"Start training!\\n\")\n",
        "\n",
        "    print(f\"{'Epoch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {\\\n",
        "    'Val Acc':^9} | {'Elapsed':^9}\")\n",
        "    print(\"-\"*60)\n",
        "\n",
        "    for epoch_i in range(epochs):\n",
        "\n",
        "\n",
        "        ################## Training ##############\n",
        "\n",
        "        t0_epoch = time.time()\n",
        "        total_loss = 0\n",
        "\n",
        "        # training mode\n",
        "        model.train()\n",
        "\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            # Load batch to GPU\n",
        "            tr_input_ids, tr_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "            model.zero_grad()\n",
        "            pred_grad = model(tr_input_ids)\n",
        "\n",
        "            loss = loss_fn(pred_grad, tr_labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "        # Calculate the average loss over the entire training data\n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "        ################## Evaluation #######################\n",
        "\n",
        "        if val_dataloader is not None:\n",
        "\n",
        "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
        "\n",
        "            # best accuracy\n",
        "            if val_accuracy > best_accuracy:\n",
        "                best_accuracy = val_accuracy\n",
        "\n",
        "            time_elapsed = time.time() - t0_epoch\n",
        "            print(f\"{epoch_i + 1:^7} | {avg_train_loss:^12.6f} | {\\\n",
        "            val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
        "            \n",
        "    print(\"\\n\")\n",
        "    print(f\"Training complete! Best accuracy: {best_accuracy:.2f}%.\")\n",
        "\n",
        "def evaluate(model, val_dataloader):\n",
        "    \"\"\"After the completion of each training epoch, measure the model's\n",
        "    performance on our validation set.\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    val_accuracy = []\n",
        "    val_loss = []\n",
        "\n",
        "    for batch in val_dataloader:\n",
        "        ev_input_ids, ev_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(ev_input_ids)\n",
        "\n",
        "        loss = loss_fn(logits, ev_labels)\n",
        "        val_loss.append(loss.item())\n",
        "\n",
        "        preds = torch.argmax(logits, dim=1).flatten()\n",
        "\n",
        "        # Accuracy rate from mean\n",
        "        accuracy = (preds == ev_labels).cpu().numpy().mean() * 100\n",
        "        val_accuracy.append(accuracy)\n",
        "\n",
        "    val_loss = np.mean(val_loss)\n",
        "    val_accuracy = np.mean(val_accuracy)\n",
        "\n",
        "    return val_loss, val_accuracy"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_saLGDCJDhTv",
        "outputId": "45059cbe-11f8-4f4d-883c-ff66f725de23"
      },
      "source": [
        "# are randomly initialized word vectors\n",
        "cnn_rand, optimizer = initialize_model(vocab_size=len(word2idx),\n",
        "                                      embed_dim=300,\n",
        "                                      learning_rate=0.25,\n",
        "                                      dropout=0.5)\n",
        "train(cnn_rand, optimizer, train_dataloader, val_dataloader, epochs=20)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training!\n",
            "\n",
            " Epoch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "------------------------------------------------------------\n",
            "   1    |  91.785452   | 172.627165 |   55.71   |   3.26   \n",
            "   2    |  339.568625  | 254.412391 |   80.00   |   2.92   \n",
            "   3    |  255.369074  | 302.818338 |   75.14   |   2.91   \n",
            "   4    |  321.280829  | 498.296321 |   78.86   |   2.91   \n",
            "   5    |  330.805550  | 213.505090 |   77.43   |   2.91   \n",
            "   6    |  276.886445  | 413.625556 |   74.00   |   2.91   \n",
            "   7    |  227.664535  | 294.670962 |   63.14   |   2.92   \n",
            "   8    |  208.666763  | 291.935737 |   60.00   |   2.91   \n",
            "   9    |  209.746330  | 505.166526 |   66.57   |   2.91   \n",
            "  10    |  207.079414  | 422.051998 |   74.57   |   2.91   \n",
            "  11    |  171.305536  | 389.187895 |   80.86   |   2.92   \n",
            "  12    |  120.284912  | 345.973269 |   78.29   |   2.91   \n",
            "  13    |  85.493789   | 290.588449 |   72.00   |   2.92   \n",
            "  14    |  56.585863   | 314.023468 |   77.14   |   2.91   \n",
            "  15    |  65.184303   | 462.968924 |   78.29   |   2.91   \n",
            "  16    |  79.119565   | 283.797189 |   76.86   |   2.91   \n",
            "  17    |  63.882118   | 563.441110 |   64.57   |   2.91   \n",
            "  18    |  102.267731  | 908.734576 |   73.14   |   2.91   \n",
            "  19    |  91.439228   | 1229.735892 |   76.29   |   2.90   \n",
            "  20    |  100.867204  | 600.091893 |   65.14   |   2.91   \n",
            "\n",
            "\n",
            "Training complete! Best accuracy: 80.86%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qf2RBV_5EEkw",
        "outputId": "ebb1fc9e-fe46-46b5-b6e3-7024658d5ab8"
      },
      "source": [
        "# freezed pretrained word vectors during training\n",
        "set_seed(28)\n",
        "cnn_static, optimizer = initialize_model(pretrained_embedding=embeddings,\n",
        "                                        freeze_embedding=True,\n",
        "                                        learning_rate=0.25,\n",
        "                                        dropout=0.5)\n",
        "train(cnn_static, optimizer, train_dataloader, val_dataloader, epochs=20)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training!\n",
            "\n",
            " Epoch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "------------------------------------------------------------\n",
            "   1    |  25.289047   | 16.177812  |   80.00   |   2.23   \n",
            "   2    |  17.198512   | 13.794619  |   71.71   |   2.23   \n",
            "   3    |  15.973351   | 13.783783  |   74.57   |   2.23   \n",
            "   4    |  16.085446   | 15.099487  |   68.57   |   2.23   \n",
            "   5    |  19.292009   | 13.183061  |   77.43   |   2.23   \n",
            "   6    |  14.438902   | 15.779487  |   75.43   |   2.23   \n",
            "   7    |  11.495174   | 19.167115  |   72.29   |   2.23   \n",
            "   8    |  19.777563   | 12.532807  |   74.29   |   2.23   \n",
            "   9    |   9.793668   | 16.077964  |   76.86   |   2.23   \n",
            "  10    |   5.951348   | 10.447345  |   75.71   |   2.23   \n",
            "  11    |   5.084112   | 14.033642  |   77.71   |   2.23   \n",
            "  12    |   4.019488   | 19.637546  |   78.86   |   2.23   \n",
            "  13    |   4.493736   | 15.647094  |   73.71   |   2.23   \n",
            "  14    |   3.936853   | 14.716355  |   78.00   |   2.23   \n",
            "  15    |   2.415392   | 16.786713  |   77.71   |   2.23   \n",
            "  16    |   2.029644   | 14.049903  |   76.86   |   2.23   \n",
            "  17    |   2.121284   | 15.284916  |   77.43   |   2.23   \n",
            "  18    |   3.022773   | 14.245349  |   76.86   |   2.23   \n",
            "  19    |  10.239847   | 31.881574  |   75.71   |   2.23   \n",
            "  20    |   4.883274   | 15.563198  |   74.57   |   2.23   \n",
            "\n",
            "\n",
            "Training complete! Best accuracy: 80.00%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2UmlxJSEHvP",
        "outputId": "e325c7c7-0020-4ed8-887e-dfcf2776d3f0"
      },
      "source": [
        "# fine-tuned pretrained word vectors during training \n",
        "set_seed(42)\n",
        "cnn_non_static, optimizer = initialize_model(pretrained_embedding=embeddings,\n",
        "                                            freeze_embedding=False,\n",
        "                                            learning_rate=0.25,\n",
        "                                            dropout=0.5)\n",
        "train(cnn_non_static, optimizer, train_dataloader, val_dataloader, epochs=20)\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training!\n",
            "\n",
            " Epoch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "------------------------------------------------------------\n",
            "   1    |  491.236342  | 1335.325431 |   79.14   |   3.04   \n",
            "   2    | 1804.609296  | 3654.072562 |   79.14   |   3.04   \n",
            "   3    | 3864.533686  | 3404.061663 |   66.86   |   3.04   \n",
            "   4    | 1908.577060  | 6753.296193 |   79.43   |   3.03   \n",
            "   5    | 1572.432608  | 5696.997907 |   78.57   |   3.04   \n",
            "   6    | 1606.129302  | 7033.569196 |   74.00   |   3.04   \n",
            "   7    | 1152.709840  | 8630.073661 |   68.86   |   3.04   \n",
            "   8    |  807.369789  | 8493.697998 |   74.86   |   3.03   \n",
            "   9    |  454.371077  | 17741.472726 |   78.86   |   3.04   \n",
            "  10    |  345.248887  | 14051.659681 |   78.00   |   3.04   \n",
            "  11    |  272.562099  | 14266.424953 |   78.57   |   3.04   \n",
            "  12    |  221.431018  | 15682.693045 |   78.00   |   3.03   \n",
            "  13    | 1041.820661  | 38128.037319 |   76.00   |   3.04   \n",
            "  14    | 1395.069425  | 38855.609515 |   78.29   |   3.04   \n",
            "  15    | 1786.243065  | 33854.549456 |   78.29   |   3.03   \n",
            "  16    | 1721.747922  | 34153.655971 |   77.71   |   3.04   \n",
            "  17    | 2301.558879  | 60345.178990 |   75.71   |   3.03   \n",
            "  18    | 2204.883721  | 59368.016741 |   77.71   |   3.04   \n",
            "  19    | 4026.132817  | 72380.167062 |   76.86   |   3.04   \n",
            "  20    | 4003.450958  | 66094.351702 |   66.57   |   3.03   \n",
            "\n",
            "\n",
            "Training complete! Best accuracy: 79.43%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzcYsfLpEgd3"
      },
      "source": [
        "def predict(text, model=cnn_non_static.to(\"cpu\"), max_len=62):\n",
        "    \"\"\"Predict probability based on the trained model.\"\"\"\n",
        "\n",
        "    # Tokenize, pad and encode text\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    padded_tokens = tokens + ['<pad>'] * (max_len - len(tokens))\n",
        "    input_id = [word2idx.get(token, word2idx['<unk>']) for token in padded_tokens]\n",
        "\n",
        "    # Convert to PyTorch tensors\n",
        "    input_id = torch.tensor(input_id).unsqueeze(dim=0)\n",
        "\n",
        "    # Compute logits\n",
        "    predicted = model.forward(input_id)\n",
        "\n",
        "    probs = F.softmax(predicted, dim=1).squeeze(dim=0)\n",
        "\n",
        "    print(f\"This sentence has asset index as {probs[1] }\")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxQ6XFSSJJAh",
        "outputId": "a5ba71e6-cb2d-4eb8-ca4c-0749ec9a017b"
      },
      "source": [
        "predict(\"The lower dam will regulate outflows from the 횉etin main dam and also produce hydroelectric power with a 112 MW capacity via two 56 MW Kaplan turbines.  .\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This sentence has asset index as 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JE2-7KbEJVhn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}